# pipeline.toml -- Pipeline configuration for the Thrum engine.
#
# Defines gates, budget, agent roles, sandbox, and subsampling settings.
# For a working multi-repo example, see examples/pulseengine/pipeline.toml.
# For a minimal single-repo setup, see examples/minimal/pipeline.toml.

# ── Engine Concurrency ──────────────────────────────────────────────
# When > 1, each agent gets an isolated git worktree so that multiple
# agents can work concurrently on the same repo without index conflicts.

[engine]
per_repo_limit = 4
worktrees_dir = "worktrees"
max_retries = 10  # Reset via dashboard retry button to give a task another round

# ── Gate 1: Code Quality ──────────────────────────────────────────────
# Runs fmt, clippy, and test checks against each repo.

[gates.quality]
description = "Gate 1: Code quality (fmt, clippy, test)"
checks = ["cargo_fmt", "cargo_clippy", "cargo_test"]
required = true

# ── Gate 2: Formal Verification ───────────────────────────────────────
# Runs verify_cmd and proofs_cmd from repo config, if defined.
# Set required = false if your project does not use formal verification.

[gates.proof]
description = "Gate 2: Formal verification"
checks = []
required = false

# ── Gate 3: Integration Testing ───────────────────────────────────────
# For multi-repo pipelines, define integration steps that chain outputs.
# Each step runs in the named repo's directory with placeholder substitution:
#   {fixture}    -> path to the test fixture file
#   {output_dir} -> temporary directory for pipeline artifacts

[gates.integration]
description = "Gate 3: Cross-repo integration testing"
required = false

# Uncomment and customize for your pipeline:
# fixture = "fixtures/integration_test.dat"
#
# [[gates.integration.steps]]
# repo = "frontend"
# label = "frontend build"
# args = ["build", "{fixture}", "-o", "{output_dir}/intermediate"]
#
# [[gates.integration.steps]]
# repo = "backend"
# label = "backend process"
# args = ["process", "{output_dir}/intermediate", "-o", "{output_dir}/output"]

# ── Release Configuration ─────────────────────────────────────────────

[release]
targets = [
    "aarch64-apple-darwin",
    "x86_64-unknown-linux-gnu",
]

[release.artifacts]
verification_report = true
test_report = true
checksums = "sha256"
# proof_build_log = true
# traceability_matrix = true

# ── Budget ────────────────────────────────────────────────────────────
# Overall spending ceiling and per-session timeout for AI agents.

[budget]
ceiling_usd = 2000.0
per_session_timeout_secs = 600

[budget.allocation]
implementation_opus = 600.0
planning_opus = 100.0
reviews_sonnet = 100.0
integration_sonnet = 100.0
buffer = 100.0

# ── Backends ─────────────────────────────────────────────────────────
# Register AI backends (coding agents and chat APIs).
# Any tool that accepts a prompt and returns output can be plugged in.
#
# type = "agent" → CLI-based, can edit files, run commands, use git
# type = "chat"  → API-based, returns text only (reviews, planning)
#
# Role resolution: roles reference backends by name or model substring.
# e.g., backend = "opus" matches any backend whose model contains "opus".

[[backends]]
name = "claude-code"
type = "agent"
command = "claude"
prompt_args = ["-p", "{prompt}", "--output-format", "json"]
model = "claude-opus-4-6"
timeout_secs = 1200
enabled = true

# Uncomment to add OpenCode as an alternative agent:
# [[backends]]
# name = "opencode"
# type = "agent"
# command = "opencode"
# prompt_args = ["-m", "{prompt}"]
# model = "devstral-small-2505"
# enabled = true

# Uncomment to add Aider as an alternative agent:
# [[backends]]
# name = "aider"
# type = "agent"
# command = "aider"
# prompt_args = ["--message", "{prompt}", "--yes"]
# model = "gpt-4o"
# enabled = true

[[backends]]
name = "anthropic-api"
type = "chat"
provider = "anthropic"
model = "claude-sonnet-4-5-20250929"
api_key_env = "ANTHROPIC_API_KEY"
enabled = true

# Uncomment for Mistral/Devstral:
# [[backends]]
# name = "mistral-api"
# type = "chat"
# provider = "mistral"
# model = "devstral-small-2505"
# api_key_env = "MISTRAL_API_KEY"
# enabled = true

# Uncomment for OpenAI:
# [[backends]]
# name = "openai-api"
# type = "chat"
# provider = "openai"
# model = "gpt-4o"
# api_key_env = "OPENAI_API_KEY"
# enabled = true

# ── Agent Roles ───────────────────────────────────────────────────────
# Map pipeline stages to AI backends and prompt templates.
# Backend values reference a registered backend by name or model substring.
# e.g., "opus" resolves to any backend whose model contains "opus".

[roles.implementer]
backend = "opus"
prompt_template = "agents/implementer.md"
budget_usd = 6.0
timeout_secs = 1200

[roles.reviewer]
backend = "sonnet"
prompt_template = "agents/reviewer.md"
budget_usd = 1.0
timeout_secs = 300

[roles.planner]
backend = "opus"
prompt_template = "agents/planner.md"
budget_usd = 1.0
timeout_secs = 300

[roles.ci_fixer]
backend = "opus"
prompt_template = "agents/ci_fixer.md"
budget_usd = 3.0
timeout_secs = 600

# ── Sandbox ───────────────────────────────────────────────────────────
# Resource limits for agent subprocess execution.
# backend:
#   "none"      — no isolation (passthrough)
#   "os-native" — enforce seatbelt (macOS) / bubblewrap (Linux)
#   "observe"   — run without enforcement, audit writes after execution
#                  and log which operations WOULD be denied. Useful for
#                  debugging sandbox profiles before enabling enforcement.
#   "docker"    — Docker container isolation

[sandbox]
backend = "os-native"
memory_limit_mb = 4096
cpu_limit = 2.0
network = true

# ── Subsampling ───────────────────────────────────────────────────────
# Run a fraction of gate checks to speed up iteration.
# gate1_ratio: fraction of quality checks to run (0.0-1.0)
# gate2_ratio: fraction of proof checks to run (0.0-1.0)
# gate3_ratio: fraction of integration checks to run (0.0-1.0)
# seed_strategy: "task-id" (deterministic per task) or "random"

[subsample]
enabled = false
gate1_ratio = 1.0
gate2_ratio = 1.0
gate3_ratio = 1.0
seed_strategy = "task-id"
